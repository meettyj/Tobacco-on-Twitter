{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after_start_seconds:  1459468800.0\n",
      "before_end_seconds:  1556582400.0\n"
     ]
    }
   ],
   "source": [
    "# date\n",
    "after_start_date = date.fromisoformat('2016-04-01')\n",
    "before_end_date = date.fromisoformat('2019-04-30')\n",
    "after_start_seconds = (after_start_date -  date.fromisoformat('1970-01-01')).total_seconds()\n",
    "before_end_seconds = (before_end_date -  date.fromisoformat('1970-01-01')).total_seconds()\n",
    "\n",
    "print('after_start_seconds: ', after_start_seconds)\n",
    "print('before_end_seconds: ', before_end_seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve the submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPushshiftData(query, after, before, sub):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?title='+\\\n",
    "          str(query)+'&size=1000&after='+str(after)+'&before='+str(before)+'&subreddit='+str(sub)+'&sort=asc'\n",
    "#     url = 'https://api.pushshift.io/reddit/search/submission/?title='+\\\n",
    "#           str(query)+'&size=1000'+'&subreddit='+str(sub)\n",
    "#     print(url)\n",
    "    r = requests.get(url)\n",
    "#     print(r)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']\n",
    "\n",
    "def getPushshiftData_noSubreddit(query, after, before):\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission/?title='+\\\n",
    "          str(query)+'&size=1000&after='+str(after)+'&before='+str(before)+'&sort=asc'\n",
    "#     url = 'https://api.pushshift.io/reddit/search/submission/?title='+\\\n",
    "#           str(query)+'&size=1000'+'&subreddit='+str(sub)\n",
    "#     print(url)\n",
    "    r = requests.get(url)\n",
    "    print(r)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collectSubData(subm):\n",
    "    subData = list() #list to store data points\n",
    "    title = subm['title']\n",
    "    url = subm['url']\n",
    "    try:\n",
    "        flair = subm['link_flair_text']\n",
    "    except KeyError:\n",
    "        flair = \"NaN\"    \n",
    "    author = subm['author']\n",
    "#     author_fullName = subm['author_fullname']\n",
    "    sub_id = subm['id']\n",
    "    score = subm['score']\n",
    "    try:\n",
    "        subreddit = subm['subreddit']\n",
    "    except KeyError:\n",
    "        subreddit = \"None\"\n",
    "        \n",
    "    try:\n",
    "        self_text = subm['selftext']\n",
    "    except KeyError:\n",
    "        self_text = \"None\"\n",
    "        \n",
    "    created = datetime.datetime.fromtimestamp(subm['created_utc']) #1520561700.0\n",
    "    numComms = subm['num_comments']\n",
    "    permalink = subm['permalink']\n",
    "    \n",
    "    subData.append((sub_id, title, self_text, url, author, score,subreddit, created,numComms,permalink,flair))\n",
    "    subStats[sub_id] = subData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keywords list\n",
    "# keywords = ['vape', 'e liquid', 'e juice', 'ejuice', 'electronic cigarette', 'eliquid', 'e cig', 'ecig', 'e hookah', 'electronic cig', 'ehookah']\n",
    "keywords = ['New Jersey']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search keyword without specific subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # query = \"electronic-cigarette\"\n",
    "# # sub = \"electronic_cigarette\"\n",
    "\n",
    "# for query in keywords:\n",
    "#     print('--------------------------------------')\n",
    "#     print('Query: ', query)\n",
    "#     after = str(after_start_seconds).split('.')[0]\n",
    "#     before = str(before_end_seconds).split('.')[0]\n",
    "\n",
    "#     subStats = {}\n",
    "#     subreddit_dict = {}\n",
    "\n",
    "#     data = getPushshiftData_noSubreddit(query, after, before)\n",
    "#     # Will run until all posts have been gathered \n",
    "#     # from the 'after' date up until before date\n",
    "#     while len(data) > 0:\n",
    "#         for submission in data:\n",
    "# #             print(submission)\n",
    "# #             print(submission['title'])\n",
    "# #             print(submission['subreddit'])\n",
    "# #             print('-----------')\n",
    "#             collectSubData(submission)\n",
    "#     #         print(submission)\n",
    "#             try:\n",
    "#                 subreddit = submission['subreddit']\n",
    "#             except KeyError:\n",
    "#                 subreddit = \"None\"\n",
    "#             subreddit_dict[subreddit] = subreddit_dict.get(subreddit, 0) + 1\n",
    "#     #         break\n",
    "\n",
    "#         # Prepare for next iteration\n",
    "#         # print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "#         after = data[-1]['created_utc']\n",
    "#         data = getPushshiftData_noSubreddit(query, after, before)\n",
    "\n",
    "#     print(len(data))\n",
    "#     print('length of subStats.keys(): ', len(subStats.keys()))\n",
    "# #     print('subreddit_dict: ', subreddit_dict)\n",
    "#     print('Sorted: ', sorted(subreddit_dict.items(), key=lambda d: d[1], reverse=True))\n",
    "#     break\n",
    "# #     print('Sorted: ', sorted(subreddit_dict.items(), key=lambda d: d[1], reverse=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific subreddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'author': 'Boulanger97', 'author_created_utc': 1430364051, 'author_flair_css_class': None, 'author_flair_text': None, 'author_fullname': 't2_n7f9z', 'created_utc': 1459468841, 'domain': 'self.electronic_cigarette', 'full_link': 'https://www.reddit.com/r/electronic_cigarette/comments/4cstzt/bulged_moonshot_pyrex/', 'gilded': 0, 'id': '4cstzt', 'is_self': True, 'locked': False, 'media_embed': {}, 'num_comments': 10, 'over_18': False, 'permalink': '/r/electronic_cigarette/comments/4cstzt/bulged_moonshot_pyrex/', 'retrieved_on': 1463511566, 'score': 2, 'secure_media_embed': {}, 'selftext': 'I saw a post a few weeks ago someone bought bulged pyrex for the moonshot rdta. Does anyone know where I could buy that?', 'stickied': False, 'subreddit': 'electronic_cigarette', 'subreddit_id': 't5_2qmlu', 'thumbnail': 'self', 'title': 'Bulged Moonshot Pyrex', 'url': 'https://www.reddit.com/r/electronic_cigarette/comments/4cstzt/bulged_moonshot_pyrex/'}\n",
      "1000\n",
      "2016-04-05 04:06:59\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# query = \"\"\n",
    "# sub = \"electronic_cigarette\"\n",
    "# after = str(after_start_seconds).split('.')[0]\n",
    "# before = str(before_end_seconds).split('.')[0]\n",
    "\n",
    "# data = getPushshiftData(query, after, before, sub)\n",
    "# # Will run until all posts have been gathered \n",
    "# # from the 'after' date up until before date\n",
    "# while len(data) > 0:\n",
    "#     for submission in data:      \n",
    "# #         collectSubData(submission)\n",
    "#         print(submission)\n",
    "# #         subCount+=1\n",
    "#         break\n",
    "#     # Calls getPushshiftData() with the created date of the last submission\n",
    "#     print(len(data))\n",
    "#     print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "#     after = data[-1]['created_utc']\n",
    "#     data = getPushshiftData(query, after, before, sub)\n",
    "#     break\n",
    "    \n",
    "# print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2016-04-05 04:06:59\n",
      "1000\n",
      "2016-04-08 23:38:06\n",
      "1000\n",
      "2016-04-13 07:23:50\n",
      "1000\n",
      "2016-04-17 16:44:39\n",
      "1000\n",
      "2016-04-21 18:52:29\n",
      "1000\n",
      "2016-04-26 12:16:57\n",
      "1000\n",
      "2016-04-30 04:41:07\n",
      "1000\n",
      "2016-05-04 13:55:34\n",
      "1000\n",
      "2016-05-08 04:20:28\n",
      "1000\n",
      "2016-05-11 21:09:33\n",
      "1000\n",
      "2016-05-15 20:13:20\n",
      "1000\n",
      "2016-05-19 19:06:14\n",
      "1000\n",
      "2016-05-24 08:38:10\n",
      "1000\n",
      "2016-05-28 17:29:03\n",
      "1000\n",
      "2016-06-01 23:15:49\n",
      "1000\n",
      "2016-06-06 13:58:15\n",
      "1000\n",
      "2016-06-10 12:48:35\n",
      "1000\n",
      "2016-06-14 22:43:33\n",
      "1000\n",
      "2016-06-19 11:12:19\n",
      "1000\n",
      "2016-06-23 19:03:27\n",
      "1000\n",
      "2016-06-28 13:56:39\n",
      "1000\n",
      "2016-07-03 02:50:31\n",
      "1000\n",
      "2016-07-07 22:00:53\n",
      "1000\n",
      "2016-07-12 15:18:03\n",
      "1000\n",
      "2016-07-17 06:29:52\n",
      "1000\n",
      "2016-07-22 13:54:16\n",
      "1000\n",
      "2016-07-27 11:05:01\n",
      "1000\n",
      "2016-08-01 03:48:46\n",
      "1000\n",
      "2016-08-05 15:31:37\n",
      "1000\n",
      "2016-08-09 23:07:34\n",
      "1000\n",
      "2016-08-15 17:34:55\n",
      "1000\n",
      "2016-08-20 21:02:25\n",
      "1000\n",
      "2016-08-26 19:47:52\n",
      "1000\n",
      "2016-09-01 14:54:58\n",
      "1000\n",
      "2016-09-07 01:07:34\n",
      "1000\n",
      "2016-09-13 04:21:17\n",
      "1000\n",
      "2016-09-19 14:45:42\n",
      "1000\n",
      "2016-09-25 17:44:29\n",
      "1000\n",
      "2016-10-02 12:25:47\n",
      "1000\n",
      "2016-10-09 13:56:27\n",
      "1000\n",
      "2016-10-16 08:27:06\n",
      "1000\n",
      "2016-10-22 20:41:51\n",
      "1000\n",
      "2016-10-30 04:08:02\n",
      "1000\n",
      "2016-11-06 05:31:44\n",
      "1000\n",
      "2016-11-13 08:52:15\n",
      "1000\n",
      "2016-11-20 11:48:44\n",
      "1000\n",
      "2016-11-27 00:04:19\n",
      "1000\n",
      "2016-12-03 03:56:00\n",
      "1000\n",
      "2016-12-10 01:48:14\n",
      "1000\n",
      "2016-12-16 23:57:22\n",
      "1000\n",
      "2016-12-24 01:01:55\n",
      "1000\n",
      "2016-12-30 17:53:24\n",
      "1000\n",
      "2017-01-06 10:17:37\n",
      "1000\n",
      "2017-01-12 13:14:26\n",
      "1000\n",
      "2017-01-18 18:58:57\n",
      "1000\n",
      "2017-01-25 03:09:54\n",
      "1000\n",
      "2017-02-01 02:33:09\n",
      "1000\n",
      "2017-02-07 23:03:24\n",
      "1000\n",
      "2017-02-14 23:42:07\n",
      "1000\n",
      "2017-02-22 03:59:09\n",
      "1000\n",
      "2017-03-01 13:06:46\n",
      "1000\n",
      "2017-03-08 16:34:47\n",
      "1000\n",
      "2017-03-15 17:05:55\n",
      "1000\n",
      "2017-03-23 03:58:55\n",
      "1000\n",
      "2017-03-30 16:51:01\n",
      "1000\n",
      "2017-04-07 06:56:54\n",
      "1000\n",
      "2017-04-14 17:43:32\n",
      "1000\n",
      "2017-04-22 04:00:45\n",
      "1000\n",
      "2017-04-29 11:36:29\n",
      "1000\n",
      "2017-05-06 18:08:12\n",
      "1000\n",
      "2017-05-14 01:07:24\n",
      "1000\n",
      "2017-05-22 00:15:36\n",
      "1000\n",
      "2017-05-29 19:40:17\n",
      "1000\n",
      "2017-06-06 04:08:31\n",
      "1000\n",
      "2017-06-13 21:49:38\n",
      "1000\n",
      "2017-06-21 18:09:17\n",
      "1000\n",
      "2017-06-29 16:11:17\n",
      "1000\n",
      "2017-07-07 19:13:48\n",
      "1000\n",
      "2017-07-15 21:59:08\n",
      "1000\n",
      "2017-07-24 01:01:27\n",
      "1000\n",
      "2017-08-01 04:25:49\n",
      "1000\n",
      "2017-08-09 11:04:05\n",
      "1000\n",
      "2017-08-17 14:10:58\n",
      "1000\n",
      "2017-08-25 17:34:54\n",
      "1000\n",
      "2017-09-03 10:14:59\n",
      "1000\n",
      "2017-09-11 22:05:23\n",
      "1000\n",
      "2017-09-20 14:28:13\n",
      "1000\n",
      "2017-09-29 02:31:22\n",
      "1000\n",
      "2017-10-07 22:23:00\n",
      "1000\n",
      "2017-10-16 16:44:54\n",
      "1000\n",
      "2017-10-24 10:48:26\n",
      "1000\n",
      "2017-10-31 22:51:59\n",
      "1000\n",
      "2017-11-09 01:41:51\n",
      "1000\n",
      "2017-11-17 02:57:58\n",
      "1000\n",
      "2017-11-24 18:34:12\n",
      "1000\n",
      "2017-12-02 00:54:51\n",
      "1000\n",
      "2017-12-10 06:22:05\n",
      "1000\n",
      "2017-12-18 21:07:53\n",
      "1000\n",
      "2017-12-27 23:12:40\n",
      "1000\n",
      "2018-01-05 19:54:36\n",
      "1000\n",
      "2018-01-13 22:31:53\n",
      "1000\n",
      "2018-01-21 21:48:06\n",
      "1000\n",
      "2018-01-29 20:24:59\n",
      "1000\n",
      "2018-02-06 19:55:31\n",
      "1000\n",
      "2018-02-14 23:43:10\n",
      "1000\n",
      "2018-02-23 07:53:24\n",
      "1000\n",
      "2018-03-03 19:13:44\n",
      "1000\n",
      "2018-03-12 20:42:56\n",
      "1000\n",
      "2018-03-21 14:17:24\n",
      "1000\n",
      "2018-03-30 03:10:47\n",
      "1000\n",
      "2018-04-08 18:31:48\n",
      "1000\n",
      "2018-04-17 17:57:21\n",
      "1000\n",
      "2018-04-26 19:31:40\n",
      "1000\n",
      "2018-05-06 16:32:56\n",
      "1000\n",
      "2018-05-15 22:08:49\n",
      "1000\n",
      "2018-05-26 02:07:53\n",
      "1000\n",
      "2018-06-05 11:35:04\n",
      "1000\n",
      "2018-06-15 01:40:47\n",
      "1000\n",
      "2018-06-25 19:07:11\n",
      "1000\n",
      "2018-07-05 08:50:51\n",
      "1000\n",
      "2018-07-15 12:21:44\n",
      "1000\n",
      "2018-07-25 02:47:57\n",
      "1000\n",
      "2018-08-04 05:22:11\n",
      "1000\n",
      "2018-08-14 21:18:53\n",
      "1000\n",
      "2018-08-25 23:27:20\n",
      "1000\n",
      "2018-09-06 22:00:06\n",
      "1000\n",
      "2018-09-18 10:12:19\n",
      "1000\n",
      "2018-09-29 01:28:06\n",
      "1000\n",
      "2018-10-09 23:34:09\n",
      "1000\n",
      "2018-10-19 22:14:09\n",
      "1000\n",
      "2018-10-31 03:10:14\n",
      "1000\n",
      "2018-11-10 22:18:19\n",
      "1000\n",
      "2018-11-21 20:54:18\n",
      "1000\n",
      "2018-12-02 05:49:15\n",
      "1000\n",
      "2018-12-13 04:26:17\n",
      "1000\n",
      "2018-12-25 23:50:42\n",
      "1000\n",
      "2019-01-06 20:34:58\n",
      "1000\n",
      "2019-01-17 12:37:36\n",
      "1000\n",
      "2019-01-28 01:36:37\n",
      "1000\n",
      "2019-02-07 23:49:38\n",
      "1000\n",
      "2019-02-18 14:02:16\n",
      "1000\n",
      "2019-02-28 19:16:57\n",
      "1000\n",
      "2019-03-11 17:05:20\n",
      "1000\n",
      "2019-03-21 12:00:49\n",
      "1000\n",
      "2019-04-01 10:09:35\n",
      "1000\n",
      "2019-04-11 06:45:45\n",
      "1000\n",
      "2019-04-22 18:39:21\n",
      "696\n",
      "2019-04-29 23:37:55\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\n",
    "sub = \"electronic_cigarette\"\n",
    "after = str(after_start_seconds).split('.')[0]\n",
    "before = str(before_end_seconds).split('.')[0]\n",
    "\n",
    "\n",
    "subCount = 0\n",
    "subStats = {}\n",
    "\n",
    "data = getPushshiftData(query, after, before, sub)\n",
    "# Will run until all posts have been gathered \n",
    "# from the 'after' date up until before date\n",
    "while len(data) > 0:\n",
    "    for submission in data:      \n",
    "        collectSubData(submission)\n",
    "#         print(submission)\n",
    "#         subCount+=1\n",
    "#         break\n",
    "    # Calls getPushshiftData() with the created date of the last submission\n",
    "    print(len(data))\n",
    "    print(str(datetime.datetime.fromtimestamp(data[-1]['created_utc'])))\n",
    "    after = data[-1]['created_utc']\n",
    "    data = getPushshiftData(query, after, before, sub)\n",
    "#     break\n",
    "    \n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of subStats.keys():  147696\n"
     ]
    }
   ],
   "source": [
    "print('length of subStats.keys(): ', len(subStats.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147696 submissions have added to list\n",
      "1st entry is:\n",
      "Bulged Moonshot Pyrex created: 2\n",
      "Last entry is:\n",
      "I am so tired of my Aspire breeze 1 created: 2\n"
     ]
    }
   ],
   "source": [
    "print(str(len(subStats)) + \" submissions have added to list\")\n",
    "print(\"1st entry is:\")\n",
    "print(list(subStats.values())[0][0][1] + \" created: \" + str(list(subStats.values())[0][0][5]))\n",
    "print(\"Last entry is:\")\n",
    "print(list(subStats.values())[-1][0][1] + \" created: \" + str(list(subStats.values())[-1][0][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147696 submissions have been uploaded\n"
     ]
    }
   ],
   "source": [
    "def updateSubs_file(fileName, subStats):\n",
    "    upload_count = 0\n",
    "    location = \"/mnt/volume-second-5T/subreddits/\"\n",
    "#     print(\"input filename of submission file, please add .csv\")\n",
    "#     filename = input()\n",
    "    fileName = fileName + \".csv\"\n",
    "    file = location + fileName\n",
    "    with open(file, 'w', newline='', encoding='utf-8') as file: \n",
    "        a = csv.writer(file, delimiter=',')\n",
    "        headers = [\"Post_ID\",\"Title\",\"Self_text\", \"Url\",\"Author\",\"Score\",\"Subreddit\",\"Publish_date\",\"Number_of_comments\",\"Permalink\",\"Flair\"]\n",
    "        a.writerow(headers)\n",
    "        for sub in subStats:\n",
    "            a.writerow(subStats[sub][0])\n",
    "            upload_count+=1\n",
    "            \n",
    "        print(str(upload_count) + \" submissions have been uploaded\")\n",
    "        \n",
    "updateSubs_file(\"electronic_cigarette\", subStats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
